{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "\n",
    "\n",
    "# jupyter配置\n",
    "pd.options.display.max_rows=1000 #Notebook 的一个cell的显示行数\n",
    "pd.options.display.max_columns=10000#Notebook 的一个cell的显示列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')\n",
    "train_data_y= train_data.loc[:, 'rating'] - 1\n",
    "train_data_x = train_data.loc[:, [\"user_id\", \"product_id\"]] # \"product_name\", \"votes\", \"helpful_votes\"\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "test_data = test_data.loc[:, [\"user_id\", \"product_id\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_x, train_data_y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(745889, 7) (522122, 2) (223767, 2) (522122,) (223767,) (223553, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id       int64\n",
       "product_id    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x.describe()\n",
    "train_data_x.dtypes\n",
    "test_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324619    4\n",
       "583405    1\n",
       "469608    1\n",
       "137583    2\n",
       "250603    4\n",
       "         ..\n",
       "347863    2\n",
       "17546     4\n",
       "548894    3\n",
       "347802    4\n",
       "219883    2\n",
       "Name: rating, Length: 522122, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.54732\ttest-mlogloss:1.54743\n",
      "[1]\ttrain-mlogloss:1.49565\ttest-mlogloss:1.49593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wzq/anaconda3/envs/torch/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [17:38:21] WARNING: /var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_d9k8pmaj4_/croot/xgboost-split_1724073758172/work/src/learner.cc:740: \n",
      "Parameters: { \"enable_categorical\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\ttrain-mlogloss:1.45189\ttest-mlogloss:1.45232\n",
      "[3]\ttrain-mlogloss:1.41450\ttest-mlogloss:1.41506\n",
      "[4]\ttrain-mlogloss:1.38223\ttest-mlogloss:1.38295\n",
      "[5]\ttrain-mlogloss:1.35412\ttest-mlogloss:1.35499\n",
      "[6]\ttrain-mlogloss:1.32963\ttest-mlogloss:1.33066\n",
      "[7]\ttrain-mlogloss:1.30821\ttest-mlogloss:1.30940\n",
      "[8]\ttrain-mlogloss:1.28941\ttest-mlogloss:1.29078\n",
      "[9]\ttrain-mlogloss:1.27285\ttest-mlogloss:1.27439\n",
      "[10]\ttrain-mlogloss:1.25817\ttest-mlogloss:1.25990\n",
      "[11]\ttrain-mlogloss:1.24519\ttest-mlogloss:1.24710\n",
      "[12]\ttrain-mlogloss:1.23367\ttest-mlogloss:1.23577\n",
      "[13]\ttrain-mlogloss:1.22338\ttest-mlogloss:1.22569\n",
      "[14]\ttrain-mlogloss:1.21422\ttest-mlogloss:1.21674\n",
      "[15]\ttrain-mlogloss:1.20606\ttest-mlogloss:1.20879\n",
      "[16]\ttrain-mlogloss:1.19871\ttest-mlogloss:1.20163\n",
      "[17]\ttrain-mlogloss:1.19221\ttest-mlogloss:1.19531\n",
      "[18]\ttrain-mlogloss:1.18632\ttest-mlogloss:1.18960\n",
      "[19]\ttrain-mlogloss:1.18105\ttest-mlogloss:1.18453\n",
      "[20]\ttrain-mlogloss:1.17629\ttest-mlogloss:1.17996\n",
      "[21]\ttrain-mlogloss:1.17196\ttest-mlogloss:1.17584\n",
      "[22]\ttrain-mlogloss:1.16809\ttest-mlogloss:1.17218\n",
      "[23]\ttrain-mlogloss:1.16455\ttest-mlogloss:1.16885\n",
      "[24]\ttrain-mlogloss:1.16134\ttest-mlogloss:1.16583\n",
      "[25]\ttrain-mlogloss:1.15842\ttest-mlogloss:1.16311\n",
      "[26]\ttrain-mlogloss:1.15580\ttest-mlogloss:1.16067\n",
      "[27]\ttrain-mlogloss:1.15342\ttest-mlogloss:1.15847\n",
      "[28]\ttrain-mlogloss:1.15131\ttest-mlogloss:1.15652\n",
      "[29]\ttrain-mlogloss:1.14931\ttest-mlogloss:1.15470\n",
      "[30]\ttrain-mlogloss:1.14739\ttest-mlogloss:1.15301\n",
      "[31]\ttrain-mlogloss:1.14553\ttest-mlogloss:1.15137\n",
      "[32]\ttrain-mlogloss:1.14385\ttest-mlogloss:1.14990\n",
      "[33]\ttrain-mlogloss:1.14221\ttest-mlogloss:1.14845\n",
      "[34]\ttrain-mlogloss:1.14077\ttest-mlogloss:1.14722\n",
      "[35]\ttrain-mlogloss:1.13952\ttest-mlogloss:1.14615\n",
      "[36]\ttrain-mlogloss:1.13827\ttest-mlogloss:1.14508\n",
      "[37]\ttrain-mlogloss:1.13709\ttest-mlogloss:1.14409\n",
      "[38]\ttrain-mlogloss:1.13605\ttest-mlogloss:1.14323\n",
      "[39]\ttrain-mlogloss:1.13512\ttest-mlogloss:1.14250\n",
      "[40]\ttrain-mlogloss:1.13416\ttest-mlogloss:1.14173\n",
      "[41]\ttrain-mlogloss:1.13329\ttest-mlogloss:1.14101\n",
      "[42]\ttrain-mlogloss:1.13245\ttest-mlogloss:1.14035\n",
      "[43]\ttrain-mlogloss:1.13148\ttest-mlogloss:1.13961\n",
      "[44]\ttrain-mlogloss:1.13071\ttest-mlogloss:1.13904\n",
      "[45]\ttrain-mlogloss:1.12995\ttest-mlogloss:1.13846\n",
      "[46]\ttrain-mlogloss:1.12918\ttest-mlogloss:1.13790\n",
      "[47]\ttrain-mlogloss:1.12850\ttest-mlogloss:1.13743\n",
      "[48]\ttrain-mlogloss:1.12790\ttest-mlogloss:1.13698\n",
      "[49]\ttrain-mlogloss:1.12726\ttest-mlogloss:1.13653\n",
      "[50]\ttrain-mlogloss:1.12670\ttest-mlogloss:1.13614\n",
      "[51]\ttrain-mlogloss:1.12615\ttest-mlogloss:1.13575\n",
      "[52]\ttrain-mlogloss:1.12559\ttest-mlogloss:1.13536\n",
      "[53]\ttrain-mlogloss:1.12508\ttest-mlogloss:1.13501\n",
      "[54]\ttrain-mlogloss:1.12461\ttest-mlogloss:1.13467\n",
      "[55]\ttrain-mlogloss:1.12409\ttest-mlogloss:1.13436\n",
      "[56]\ttrain-mlogloss:1.12363\ttest-mlogloss:1.13406\n",
      "[57]\ttrain-mlogloss:1.12320\ttest-mlogloss:1.13380\n",
      "[58]\ttrain-mlogloss:1.12266\ttest-mlogloss:1.13346\n",
      "[59]\ttrain-mlogloss:1.12212\ttest-mlogloss:1.13311\n",
      "[60]\ttrain-mlogloss:1.12170\ttest-mlogloss:1.13286\n",
      "[61]\ttrain-mlogloss:1.12126\ttest-mlogloss:1.13260\n",
      "[62]\ttrain-mlogloss:1.12084\ttest-mlogloss:1.13236\n",
      "[63]\ttrain-mlogloss:1.12044\ttest-mlogloss:1.13216\n",
      "[64]\ttrain-mlogloss:1.11999\ttest-mlogloss:1.13190\n",
      "[65]\ttrain-mlogloss:1.11953\ttest-mlogloss:1.13162\n",
      "[66]\ttrain-mlogloss:1.11918\ttest-mlogloss:1.13143\n",
      "[67]\ttrain-mlogloss:1.11875\ttest-mlogloss:1.13118\n",
      "[68]\ttrain-mlogloss:1.11842\ttest-mlogloss:1.13101\n",
      "[69]\ttrain-mlogloss:1.11805\ttest-mlogloss:1.13081\n",
      "[70]\ttrain-mlogloss:1.11775\ttest-mlogloss:1.13066\n",
      "[71]\ttrain-mlogloss:1.11737\ttest-mlogloss:1.13047\n",
      "[72]\ttrain-mlogloss:1.11703\ttest-mlogloss:1.13030\n",
      "[73]\ttrain-mlogloss:1.11662\ttest-mlogloss:1.13008\n",
      "[74]\ttrain-mlogloss:1.11633\ttest-mlogloss:1.12993\n",
      "[75]\ttrain-mlogloss:1.11589\ttest-mlogloss:1.12968\n",
      "[76]\ttrain-mlogloss:1.11556\ttest-mlogloss:1.12952\n",
      "[77]\ttrain-mlogloss:1.11515\ttest-mlogloss:1.12930\n",
      "[78]\ttrain-mlogloss:1.11482\ttest-mlogloss:1.12915\n",
      "[79]\ttrain-mlogloss:1.11445\ttest-mlogloss:1.12897\n",
      "[80]\ttrain-mlogloss:1.11417\ttest-mlogloss:1.12884\n",
      "[81]\ttrain-mlogloss:1.11377\ttest-mlogloss:1.12862\n",
      "[82]\ttrain-mlogloss:1.11344\ttest-mlogloss:1.12848\n",
      "[83]\ttrain-mlogloss:1.11316\ttest-mlogloss:1.12836\n",
      "[84]\ttrain-mlogloss:1.11289\ttest-mlogloss:1.12823\n",
      "[85]\ttrain-mlogloss:1.11263\ttest-mlogloss:1.12812\n",
      "[86]\ttrain-mlogloss:1.11232\ttest-mlogloss:1.12799\n",
      "[87]\ttrain-mlogloss:1.11202\ttest-mlogloss:1.12785\n",
      "[88]\ttrain-mlogloss:1.11172\ttest-mlogloss:1.12772\n",
      "[89]\ttrain-mlogloss:1.11142\ttest-mlogloss:1.12758\n",
      "[90]\ttrain-mlogloss:1.11108\ttest-mlogloss:1.12743\n",
      "[91]\ttrain-mlogloss:1.11079\ttest-mlogloss:1.12730\n",
      "[92]\ttrain-mlogloss:1.11049\ttest-mlogloss:1.12719\n",
      "[93]\ttrain-mlogloss:1.11023\ttest-mlogloss:1.12710\n",
      "[94]\ttrain-mlogloss:1.10992\ttest-mlogloss:1.12695\n",
      "[95]\ttrain-mlogloss:1.10967\ttest-mlogloss:1.12683\n",
      "[96]\ttrain-mlogloss:1.10940\ttest-mlogloss:1.12671\n",
      "[97]\ttrain-mlogloss:1.10914\ttest-mlogloss:1.12662\n",
      "[98]\ttrain-mlogloss:1.10883\ttest-mlogloss:1.12650\n",
      "[99]\ttrain-mlogloss:1.10855\ttest-mlogloss:1.12636\n",
      "[100]\ttrain-mlogloss:1.10828\ttest-mlogloss:1.12627\n",
      "[101]\ttrain-mlogloss:1.10803\ttest-mlogloss:1.12617\n",
      "[102]\ttrain-mlogloss:1.10776\ttest-mlogloss:1.12609\n",
      "[103]\ttrain-mlogloss:1.10756\ttest-mlogloss:1.12603\n",
      "[104]\ttrain-mlogloss:1.10732\ttest-mlogloss:1.12593\n",
      "[105]\ttrain-mlogloss:1.10706\ttest-mlogloss:1.12584\n",
      "[106]\ttrain-mlogloss:1.10690\ttest-mlogloss:1.12579\n",
      "[107]\ttrain-mlogloss:1.10663\ttest-mlogloss:1.12568\n",
      "[108]\ttrain-mlogloss:1.10637\ttest-mlogloss:1.12559\n",
      "[109]\ttrain-mlogloss:1.10610\ttest-mlogloss:1.12547\n",
      "[110]\ttrain-mlogloss:1.10584\ttest-mlogloss:1.12535\n",
      "[111]\ttrain-mlogloss:1.10567\ttest-mlogloss:1.12530\n",
      "[112]\ttrain-mlogloss:1.10542\ttest-mlogloss:1.12521\n",
      "[113]\ttrain-mlogloss:1.10517\ttest-mlogloss:1.12511\n",
      "[114]\ttrain-mlogloss:1.10501\ttest-mlogloss:1.12507\n",
      "[115]\ttrain-mlogloss:1.10477\ttest-mlogloss:1.12499\n",
      "[116]\ttrain-mlogloss:1.10452\ttest-mlogloss:1.12489\n",
      "[117]\ttrain-mlogloss:1.10429\ttest-mlogloss:1.12481\n",
      "[118]\ttrain-mlogloss:1.10410\ttest-mlogloss:1.12475\n",
      "[119]\ttrain-mlogloss:1.10389\ttest-mlogloss:1.12469\n",
      "[120]\ttrain-mlogloss:1.10365\ttest-mlogloss:1.12460\n",
      "[121]\ttrain-mlogloss:1.10352\ttest-mlogloss:1.12458\n",
      "[122]\ttrain-mlogloss:1.10331\ttest-mlogloss:1.12451\n",
      "[123]\ttrain-mlogloss:1.10310\ttest-mlogloss:1.12445\n",
      "[124]\ttrain-mlogloss:1.10281\ttest-mlogloss:1.12431\n",
      "[125]\ttrain-mlogloss:1.10260\ttest-mlogloss:1.12424\n",
      "[126]\ttrain-mlogloss:1.10241\ttest-mlogloss:1.12420\n",
      "[127]\ttrain-mlogloss:1.10218\ttest-mlogloss:1.12412\n",
      "[128]\ttrain-mlogloss:1.10198\ttest-mlogloss:1.12406\n",
      "[129]\ttrain-mlogloss:1.10181\ttest-mlogloss:1.12402\n",
      "[130]\ttrain-mlogloss:1.10161\ttest-mlogloss:1.12395\n",
      "[131]\ttrain-mlogloss:1.10138\ttest-mlogloss:1.12385\n",
      "[132]\ttrain-mlogloss:1.10119\ttest-mlogloss:1.12380\n",
      "[133]\ttrain-mlogloss:1.10096\ttest-mlogloss:1.12373\n",
      "[134]\ttrain-mlogloss:1.10078\ttest-mlogloss:1.12369\n",
      "[135]\ttrain-mlogloss:1.10060\ttest-mlogloss:1.12363\n",
      "[136]\ttrain-mlogloss:1.10039\ttest-mlogloss:1.12360\n",
      "[137]\ttrain-mlogloss:1.10025\ttest-mlogloss:1.12356\n",
      "[138]\ttrain-mlogloss:1.10006\ttest-mlogloss:1.12349\n",
      "[139]\ttrain-mlogloss:1.09990\ttest-mlogloss:1.12346\n",
      "[140]\ttrain-mlogloss:1.09965\ttest-mlogloss:1.12338\n",
      "[141]\ttrain-mlogloss:1.09948\ttest-mlogloss:1.12334\n",
      "[142]\ttrain-mlogloss:1.09931\ttest-mlogloss:1.12327\n",
      "[143]\ttrain-mlogloss:1.09909\ttest-mlogloss:1.12320\n",
      "[144]\ttrain-mlogloss:1.09889\ttest-mlogloss:1.12314\n",
      "[145]\ttrain-mlogloss:1.09876\ttest-mlogloss:1.12312\n",
      "[146]\ttrain-mlogloss:1.09861\ttest-mlogloss:1.12309\n",
      "[147]\ttrain-mlogloss:1.09838\ttest-mlogloss:1.12300\n",
      "[148]\ttrain-mlogloss:1.09819\ttest-mlogloss:1.12294\n",
      "[149]\ttrain-mlogloss:1.09798\ttest-mlogloss:1.12285\n",
      "[150]\ttrain-mlogloss:1.09778\ttest-mlogloss:1.12280\n",
      "[151]\ttrain-mlogloss:1.09758\ttest-mlogloss:1.12273\n",
      "[152]\ttrain-mlogloss:1.09745\ttest-mlogloss:1.12268\n",
      "[153]\ttrain-mlogloss:1.09728\ttest-mlogloss:1.12264\n",
      "[154]\ttrain-mlogloss:1.09712\ttest-mlogloss:1.12261\n",
      "[155]\ttrain-mlogloss:1.09691\ttest-mlogloss:1.12253\n",
      "[156]\ttrain-mlogloss:1.09676\ttest-mlogloss:1.12248\n",
      "[157]\ttrain-mlogloss:1.09664\ttest-mlogloss:1.12246\n",
      "[158]\ttrain-mlogloss:1.09646\ttest-mlogloss:1.12243\n",
      "[159]\ttrain-mlogloss:1.09626\ttest-mlogloss:1.12235\n",
      "[160]\ttrain-mlogloss:1.09608\ttest-mlogloss:1.12231\n",
      "[161]\ttrain-mlogloss:1.09590\ttest-mlogloss:1.12225\n",
      "[162]\ttrain-mlogloss:1.09572\ttest-mlogloss:1.12221\n",
      "[163]\ttrain-mlogloss:1.09552\ttest-mlogloss:1.12214\n",
      "[164]\ttrain-mlogloss:1.09536\ttest-mlogloss:1.12211\n",
      "[165]\ttrain-mlogloss:1.09517\ttest-mlogloss:1.12204\n",
      "[166]\ttrain-mlogloss:1.09496\ttest-mlogloss:1.12198\n",
      "[167]\ttrain-mlogloss:1.09477\ttest-mlogloss:1.12193\n",
      "[168]\ttrain-mlogloss:1.09466\ttest-mlogloss:1.12192\n",
      "[169]\ttrain-mlogloss:1.09447\ttest-mlogloss:1.12187\n",
      "[170]\ttrain-mlogloss:1.09428\ttest-mlogloss:1.12183\n",
      "[171]\ttrain-mlogloss:1.09415\ttest-mlogloss:1.12180\n",
      "[172]\ttrain-mlogloss:1.09401\ttest-mlogloss:1.12178\n",
      "[173]\ttrain-mlogloss:1.09387\ttest-mlogloss:1.12176\n",
      "[174]\ttrain-mlogloss:1.09373\ttest-mlogloss:1.12175\n",
      "[175]\ttrain-mlogloss:1.09359\ttest-mlogloss:1.12172\n",
      "[176]\ttrain-mlogloss:1.09340\ttest-mlogloss:1.12167\n",
      "[177]\ttrain-mlogloss:1.09325\ttest-mlogloss:1.12163\n",
      "[178]\ttrain-mlogloss:1.09305\ttest-mlogloss:1.12157\n",
      "[179]\ttrain-mlogloss:1.09290\ttest-mlogloss:1.12155\n",
      "[180]\ttrain-mlogloss:1.09276\ttest-mlogloss:1.12153\n",
      "[181]\ttrain-mlogloss:1.09255\ttest-mlogloss:1.12146\n",
      "[182]\ttrain-mlogloss:1.09235\ttest-mlogloss:1.12141\n",
      "[183]\ttrain-mlogloss:1.09217\ttest-mlogloss:1.12136\n",
      "[184]\ttrain-mlogloss:1.09201\ttest-mlogloss:1.12131\n",
      "[185]\ttrain-mlogloss:1.09183\ttest-mlogloss:1.12127\n",
      "[186]\ttrain-mlogloss:1.09173\ttest-mlogloss:1.12128\n",
      "[187]\ttrain-mlogloss:1.09161\ttest-mlogloss:1.12126\n",
      "[188]\ttrain-mlogloss:1.09137\ttest-mlogloss:1.12119\n",
      "[189]\ttrain-mlogloss:1.09125\ttest-mlogloss:1.12118\n",
      "[190]\ttrain-mlogloss:1.09115\ttest-mlogloss:1.12118\n",
      "[191]\ttrain-mlogloss:1.09100\ttest-mlogloss:1.12117\n",
      "[192]\ttrain-mlogloss:1.09083\ttest-mlogloss:1.12113\n",
      "[193]\ttrain-mlogloss:1.09066\ttest-mlogloss:1.12110\n",
      "[194]\ttrain-mlogloss:1.09052\ttest-mlogloss:1.12108\n",
      "[195]\ttrain-mlogloss:1.09034\ttest-mlogloss:1.12104\n",
      "[196]\ttrain-mlogloss:1.09023\ttest-mlogloss:1.12102\n",
      "[197]\ttrain-mlogloss:1.09014\ttest-mlogloss:1.12101\n",
      "[198]\ttrain-mlogloss:1.09003\ttest-mlogloss:1.12100\n",
      "[199]\ttrain-mlogloss:1.08993\ttest-mlogloss:1.12100\n",
      "Accuracy of prediction on dataset: 0.5640197169377075\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "test_x_to_predict = xgb.DMatrix(test_data)\n",
    "\n",
    "# 设置参数\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 5,\n",
    "    'eta': 0.1,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 0.01,\n",
    "    'max_depth': 8,\n",
    "    \"enable_categorical\" : True\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "bst = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    early_stopping_rounds=10,\n",
    "    num_boost_round=200,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')] # 将训练数据和测试数据都作为验证集，可以实时监督训练情况，是否过拟合\n",
    ")\n",
    "\n",
    "# 预测结果\n",
    "result = bst.predict(\n",
    "    dtest\n",
    ")\n",
    "print('Accuracy of prediction on dataset:', accuracy_score(y_test, result))\n",
    "\n",
    "# 提交submit\n",
    "model = bst\n",
    "res = model.predict(test_x_to_predict)\n",
    "# res = np.array([np.argmax(l) for l in res])\n",
    "submit = pd.DataFrame({'ID':range(len(res)), 'label':res}).astype('int32')\n",
    "submit.to_csv(f\"./data/submit_example_A{time.strftime('%Y%m%d%H%M', time.localtime())}.csv\",index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
